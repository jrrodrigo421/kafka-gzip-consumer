# **Documenta√ß√£o da API - Kafka GZIP Consumer**

## **Descri√ß√£o do Projeto**
Esta API foi desenvolvida para consumir mensagens do Apache Kafka, salv√°-las em arquivos no formato **GZIP** no disco e persistir os metadados em um banco de dados **PostgreSQL**. A aplica√ß√£o utiliza o framework **Spring Boot** com Java 17 e possui uma rota exposta para recebimento de mensagens, permitindo a produ√ß√£o direta para o t√≥pico Kafka.

---

## **Tecnologias Utilizadas**

- **Java 17**
- **Spring Boot**
- **Apache Kafka** (Servi√ßo de Mensageria)
- **PostgreSQL** (Banco de Dados Relacional)
- **Docker** (Para execu√ß√£o local de Kafka)
- **GZIP** (Compress√£o das mensagens em arquivos)

---

## **Pr√©-requisitos**

1. **Java 17** instalado
2. **Docker** instalado e configurado
3. Servidor **PostgreSQL** configurado
4. Servi√ßo do **Kafka** em execu√ß√£o (usando Docker)

---

## **Configura√ß√£o do Projeto**

### **application.properties**

```properties
# Configura√ß√µes da Aplica√ß√£o
spring.application.name=kafkagzipconsumer 

# Configura√ß√µes do Kafka
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=kafka-gzip-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.topic=my-topic

# Configura√ß√µes para o GZIP
gzip.output-dir=./output
gzip.max-messages=5
gzip.time-interval=10000

# Configura√ß√µes do PostgreSQL
spring.datasource.url=jdbc:postgresql://localhost:5432/kafkagzip?ssl=false
spring.datasource.username=postgres
spring.datasource.password=postgres
spring.jpa.hibernate.ddl-auto=update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.show-sql=true
```

---

## **Execu√ß√£o do Projeto**

1. **Subir o Kafka utilizando Docker**:
   ```bash
   docker-compose up -d
   ```

2. **Produzir Mensagens no Kafka**:
   Execute o comando abaixo para produzir mensagens diretamente no t√≥pico **my-topic**:
   ```bash
   docker exec -it broker kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic
   ```
   ## OU
      
   ### Usar o **Frontend (Interface de Produ√ß√£o de Mensagens)**
   <br>
   
   
   üëâüèæ **[Repository](https://github.com/jrrodrigo421/kafka-producer-test)**&nbsp;&nbsp;<img src="images_readme/github.png" alt="emoji" width="20" height="20">
   
   üõú **[P√°gina Online](https://kafkaproducertest.vercel.app)**
   
   <img src="images_readme/kafka_producer_ui.jpg" alt="emoji" width="320" height="380">

   1. **Clonar o Reposit√≥rio Frontend**:
      ```bash
      git clone https://github.com/jrrodrigo421/kafka-producer-test.git
      cd kafka-producer-ui
      ```

   2. **Instalar Depend√™ncias**:
      ```bash
      npm install
      ```

   3. **Executar a Interface**:
      ```bash
      npm start
      ```
      - A interface ser√° disponibilizada em `http://localhost:3000`.

   ---



3. **Executar a API**:
   - Execute o projeto Spring Boot com o seguinte comando:
     ```bash
     mvn spring-boot:run
     ```

---

## **Rotas da API**

### **1. Rota de Produ√ß√£o de Mensagens**

- **Endpoint**: `POST /api/kafka/send`
- **Descri√ß√£o**: Permite enviar mensagens diretamente para o t√≥pico Kafka **my-topic**.
- **Par√¢metros**: 
   - **Body**: Texto simples (String)
- **Exemplo de Request**:
   ```http
   POST http://localhost:8080/api/kafka/send
   Content-Type: text/plain
   
   Esta √© uma mensagem de teste para o Kafka!
   ```

---

## **Comandos √öteis**

1. **Verificar T√≥picos do Kafka**:
   ```bash
   docker exec -it broker kafka-topics.sh --bootstrap-server localhost:9092 --list
   ```

2. **Consumir Mensagens**:
   ```bash
   docker exec -it broker kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning
   ```

---

## **Conclus√£o**
Essa documenta√ß√£o descreve como configurar, executar e interagir com a API Kafka GZIP Consumer, fornecendo um fluxo robusto e eficiente para consumo e persist√™ncia de mensagens em Kafka.

üöÄ
